*Lecture 1*

*Q1: How do we usually solve ML problems?*
***
*Lecture 2*

*Q1)A: What's 0-1 loss function?And what's the difference between it and score function?*

*Q1)B: What's the 0-1 function in a seperator way?*

*Q2: What's the Linear Logistic Classifier?*

*Q3: What does the sigmoid function mean?*

***
*Lecture 3*

*Q1: LLC in a classifier way?*

*Q2)A: How does the sigmoid function look in 1-dimensional space?*

*Q2)B: Draw σ(10x+1),σ(−2x+1) and σ(2x−3)*
***

*Note :regularization*

*Q1: What's our way to find a best linear seperator?*

*Q2: How can we describe the constant?*

*Q3: What's regularization?*
![](https://github.com/sherlcok314159/ML_learn/blob/main/Images/egularization.png)

***
*Lecture 4:*

*Q1: What does the hypotheses really do?*

*Q2: What's log loss or cross entropy loss?*

***
*Lecture 5:*

*Q1: What's the Gradient Descent algorithm?*

*Q2: When it comes to convex functions,what will happen and if not?*
***
*Lecture 6:*

*Q1: When it comes to multiple dimensions,is there any difference?*
***

*Lecture 7:*

*Q1: If step size become bigger,what will happen?Is too big better?*