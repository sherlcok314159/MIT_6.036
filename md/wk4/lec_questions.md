*Lecture 1*

*Q1: How do we usually solve ML problems?*
***
*Lecture 2*

*Q1: What's 0-1 loss function?And what's the difference between it and score function?*

*Q2: What's the Linear Logistic Classifier?*

*Q3: What does the sigmoid function mean?*

***
*Lecture 3*

*Q1: LLC in a classifier way?*

*Q2)A: How does the sigmoid function look in 1-dimensional space?*

*Q2)B: Draw σ(10x+1),σ(−2x+1) and σ(2x−3)*
***

*Note :regularization*

*Q1: What's our way to find a best linear separator?*

*Q2: How can we describe the constant?*

*Q3: What's regularization?*
![](https://github.com/sherlcok314159/ML_learn/blob/main/Images/egularization.png)

***
*Lecture 4:*

*Q1: What does the hypotheses really do?*

*Q2: What's log loss or cross entropy loss?*

***
*Lecture 5:*

*Q1: What's the Gradient Descent algorithm?*

*Q2: When it comes to convex functions,what will happen and if not?*

*Q3: What solution we can do to solve global optimal problem instead of a local one?*
***
*Lecture 6:*

*Q1: When it comes to multiple dimensions,is there any difference?*
***

*Lecture 7:*

*Q1: If step size becomes bigger,what will happen?Is too big better?*

***

*Note:Application to logistic regression objective*

*Q1: What's the linear logistic regression gradient descent?*

*Q2: What's the sgd?*

*Q3: Why is the sgd better than bgd?*

***
*Final :Tell me what you have learned in maximation margin?*